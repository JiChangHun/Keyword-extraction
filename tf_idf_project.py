# -*- coding: utf-8 -*-
"""TF-IDF Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QwqMLjTC3no8-GlCpqUfw3usJnKtSqlU

## 기본설정
"""

from google.colab import drive
drive.mount('/content/drive')

"""### 클래스 만들기"""

import json
import yaml
import re
import sys
import os

from math import log10
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer

class TF_IDF_Project:

    def __init__(self):
        self.content_list_length = 0
        self.tf_dict = dict()
        self.idf_dict = dict()
        self.df_dict = Counter()

    # config.yaml 파일 만들기
    def make_config(self):
        read_path = {'read_path': '/content/drive/MyDrive/TF-IDF_Project/2021-03-01.json'}
        output_path = {'output_path': '/content/drive/MyDrive/TF-IDF_Project/'}
        field_name = {'field_name': 'doc_content'}
        preprocessing_option = {'preprocessing_option': 2}
        output_num = {'output_num': 0}
        output_option = {'output_option': 2}

        with open('/content/drive/MyDrive/TF-IDF_Project/config.yaml', 'w') as f:
            yaml.dump(read_path, f)
            yaml.dump(output_path, f)
            yaml.dump(field_name, f)
            yaml.dump(preprocessing_option, f)
            yaml.dump(output_num, f)
            yaml.dump(output_option, f)

    # config.yaml 파일 불러오기
    def load_config(self):
        with open('/content/drive/MyDrive/TF-IDF_Project/config.yaml') as f:
            config = yaml.load(f, Loader= yaml.FullLoader)

        return config

    # 데이터 불러오기
    def load_data(self, config):
        json_list = []

        with open(config["read_path"], encoding= 'utf-8') as f:
            for jsonObj in f:
                json_data = json.loads(jsonObj)
                json_list.append(json_data)

        self.content_list_length = len(json_list)
        
        content_list = []
        id_list = []
        for index in range(len(json_list)):
            content_list.append(json_list[index].get(config["field_name"]))
            id_list.append(json_list[index].get('doc_id'))

        return content_list, id_list

    # 데이터 전처리
    def preprocess_content(self, config, content):
        # 문서 내용을 문장 단위로 분리
        # 문장 분리 조건: [!], [?], [.]
        split_content = re.split('\!|\?|\.', content)

        # 설정 옵션에 따른 텍스트 전처리
        pre_content = ''
        if config["preprocessing_option"] == 1:
            for line in split_content:
                pre_content += (re.sub('[^가-힣ㄱ-ㅣ0-9a-zA-Z]', ' ', line) + ' ')
        elif config["preprocessing_option"] == 2:
            for line in split_content:
                pre_content += (re.sub('[^가-힣ㄱ-ㅣ]', ' ', line) + ' ')
        else:
            print("preprocessing_option에 1 또는 2를 설정하세요.")
            sys.exit()

        # 2개 이상의 공백을 없애고 공백단위로 단어 분리
        space_content = ' '.join(pre_content.split())
        space_content = space_content.split()
        return space_content

    # tf.txt 만들기
    def make_tf(self, config, content_list):
        content_list_count = Counter()
        for content in content_list:
            content_list_count.update(Counter(content))
        
        all_top100_word = sorted(content_list_count.items(), key= lambda x: x[1], reverse= True)[:100]

        with open(os.path.join(config['output_path'], 'tf.txt'), 'w') as f:
            for word in all_top100_word:
                f.writelines(word[0] + " " + str(word[1]) + "\n")

    # df.txt 만들기
    def make_df(self, config, content_list):
        content_count = dict()
        top100_word = []
        for content in content_list:
            content_count = Counter(content)
            top100_word.append(sorted(content_count.items(), key= lambda x: x[1], reverse= True)[:100])

        with open(os.path.join(config['output_path'], 'df.txt'), 'w') as f:
            for line in top100_word:
                for index in range(len(line)):
                    f.writelines(str(line[index][0]) + " " + str(line[index][1]) + "\t")
                f.writelines("\n")             

    # tfidf 구하기
    def df(self, content_list):
        for content in content_list:
            self.df_dict.update(Counter(content))

    def tf(self, id, content):
        self.tf_dict[id] = Counter(content)

    def idf(self):
        self.idf_dict = self.df_dict.copy()
        for key in self.idf_dict.keys():
            self.idf_dict[key] = log10(self.content_list_length/(1 + self.df_dict[key]))

    def tf_idf(self):
        score_dict = dict()
        score_dict = self.tf_dict.copy()

        for id in score_dict.keys():
            for key in self.tf_dict[id].keys():
                score_dict[id][key] = self.tf_dict[id][key] * self.idf_dict[key]

        return score_dict

    # 가장 높은 점수를 100으로 만들기
    def score_to_100(self, score_dict):
        for id, score in score_dict.items():
            if len(score.values()) != 0:
                max_multi = 100 / max(score.values())
                for keyword, scores in score.items():
                    score_dict[id][keyword] = scores * max_multi
            else:
                continue

        return score_dict

    # diff.txt 만들기
    def max_diff(self, config, score_dict):
        diff_list = []
        for key, score in score_dict.items():
            sorted_score = []
            score_diff = 0
            sorted_score = sorted(score.values(), reverse= True)[:10]
            if len(sorted_score) != 0:
                diff = max(sorted_score) - min(sorted_score)
                diff_list.append(diff)
            else:
                diff_list.append(0)

        with open(os.path.join(config['output_path'], 'diff.txt'), 'w') as f:
            for i, key in enumerate(score_dict.keys()):
                f.writelines(key + " " + str(diff_list[i]) + "\n")

    # doc.txt 만들기
    def make_doc(self, config, score_dict):
        count_list = []
        for key, scores in score_dict.items():
            count = 0
            for score in scores.values():
                if score == 100:
                    count += 1

            count_list.append([key, count])
        count_list = sorted(count_list, key= lambda x: x[1], reverse= True)

        with open(os.path.join(config['output_path'], 'doc.txt'), 'w') as f:
            for i in range(len(count_list)):
                f.writelines(count_list[i][0] + " " + str(count_list[i][1]) + "\n")

    # long.txt 만들기
    def make_long(self, config, content_list):
        full_content = []
        for content in content_list:
            for word in content:
                full_content.append(word)

        long_top10_word = sorted(set(full_content), key= lambda x: len(x), reverse= True)[:10]

        with open(os.path.join(config['output_path'], 'long.txt'), 'w') as f:
            for word in long_top10_word:
                f.writelines(word + "\n")

    # one.txt 만들기
    def make_one(self, config, content_list):
        content_one = set()
        for content in content_list:
            for word in content:
                if len(word) == 1:
                    content_one.add(word)
                    
        with open(os.path.join(config['output_path'], 'one.txt'), 'w') as f:
                for one in content_one:
                    f.writelines(one + "\n")

    # 결과들 만들기
    def make_result(self):
        self.make_config()

        config = self.load_config()

        content_list, id_list = self.load_data(config)
        
        preprocessed_list = []
        for content in content_list:
            preprocessed_list.append(self.preprocess_content(config, content))

        # 1. 전체 문서에서 단어 개수 구하여 정렬하여 출력하기 (상위 100개)
        self.make_tf(config, preprocessed_list)

        # 2. 각 문서에 가장 많이 등장한 단어 정렬하기 출력하기(상위 100개)
        self.make_df(config, preprocessed_list[:100])

        # 3. 단어 스코어 상위 10개 기준으로 스코어 편차가 가장 많이 나는 문서 구하기
        self.df(preprocessed_list)
        self.idf()
        for id, content in zip(id_list, preprocessed_list):
            self.tf(id, content)

        score_dict = self.tf_idf()
        score_dict = self.score_to_100(score_dict)

        self.max_diff(config, score_dict)

        # 4. 스코어 100점이 가능 많은 문서대로 나열하기
        self.make_doc(config, score_dict)
        
        # 5. 전체문서에서 단어 길이가 긴 순서대로 정렬하기(상위10개)
        self.make_long(config, preprocessed_list)

        # 6. 전체문서에서 한글자 단어 출력하기
        self.make_one(config, preprocessed_list)


if __name__ == "__main__":
    tf_idf_project = TF_IDF_Project()
    tf_idf_project.make_result()

